<p align="center">
  <img src="runs/confusion_ALL9_multilayer_tau07.png" alt="Confusion Matrix" width="600">
</p>

<h1 align="center">ğŸ§¬ A Multilayer Classification Framework for Skin Disease Detection Using the SkinBench Dataset</h1>

<p align="center">
  <a href="https://pytorch.org/"><img src="https://img.shields.io/badge/PyTorch-2.1.0-EE4C2C?logo=pytorch&logoColor=white"></a>
  <a href="https://www.python.org/"><img src="https://img.shields.io/badge/Python-3.10-blue?logo=python&logoColor=white"></a>
  <a href="#"><img src="https://img.shields.io/badge/License-MIT-green.svg"></a>
  <a href="#"><img src="https://img.shields.io/badge/Status-Completed-success"></a>
</p>

---

## ğŸ§¾ Abstract

This repository contains the full training and evaluation pipeline for the research project:

> **â€œA Multilayer Classification Framework for Skin Disease Detection Using the SkinBench Dataset.â€**

It implements a **two-layer deep learning architecture** combining **ResNet-50** and **DenseNet-121**, with gating logic and performance calibration for accurate classification of **9 dermatological categories** from the **SkinBench** dataset.

---

## ğŸ“ Repository Structure

```
SkinBench/
â”œâ”€â”€ train_multilayer.py
â”œâ”€â”€ inference_multilayer.py
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ datasets.py
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ resnet_model.py
â”‚   â”œâ”€â”€ DenseNet121.py
â”‚   â”œâ”€â”€ mobilenetv3.py
â”‚   â”œâ”€â”€ cnn_model.py
â”‚   â”œâ”€â”€ hybridmodel.py
â”‚   â”œâ”€â”€ hybridSwinDenseNetMLP.py
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ eval_tools/
â”‚   â”œâ”€â”€ run_all_evals.py
â”‚   â”œâ”€â”€ dump_multilayer_preds.py
â”‚   â”œâ”€â”€ stats_mcnemar.py
â”‚   â”œâ”€â”€ xai_gradcam.py
â”‚   â”œâ”€â”€ plot_curves.py
â”‚   â”œâ”€â”€ make_my_thesis_figures.py
â”‚   â””â”€â”€ error_analysis.py
â”œâ”€â”€ raw_data/
â”œâ”€â”€ runs/ (checkpoints, figures, tables)
â””â”€â”€ requirements.txt
```

---

## ğŸ§  Dataset Overview â€” *SkinBench*


The runs/ folder is automatically generated by training and evaluation scripts. It contains the best model checkpoints along with figures and tables summarising the results. Within each phase (ALL9, L1, L2), a figures/ folder holds PNG images for:

Confusion matrices â€“ e.g. confusion_ALL9_cnn.png, confusion_ALL9_efficientnet.png.

ROC curves â€“ oneâ€‘vsâ€‘rest receiverâ€‘operating characteristic curves such as roc_ovr_ALL9_swin_densenet.png.

PR curves â€“ precisionâ€“recall curves, e.g. pr_ovr_ALL9_vit_resnet.png.

Calibration (reliability) diagrams â€“ e.g. reliability_ALL9_mobilenetv3.png.

For the binary L1 phase you will also find roc_binary_L1_resnet50.png, pr_binary_L1_resnet50.png and reliability_L1_resnet50.png.

The runs/tables/ directory collects commaâ€‘separated files summarising the experiments:

comparison_all_models.csv â€“ a leaderboard of all models and phases with accuracy, macroâ€‘F1 and AUC scores.

confusion_ALL9_*.csv â€“ raw confusion matrix counts for each ALL9 model (CNN, EfficientNet, MobileNetV3, Swinâ€‘DenseNet, VGG16/19, ViTâ€‘ResNet).

confusion_L1_resnet50.csv and confusion_L2_*.csv â€“ confusion matrices for L1 and L2 phases.

pred_ALL9_*.csv â€“ perâ€‘image predictions (probabilities and true labels) for each ALL9 model.

pred_L1_resnet50.csv and pred_L2_*.csv â€“ perâ€‘image predictions for L1 and L2 phases.

These CSV files can be opened directly in GitHub (or downloaded) to inspect the raw numbers behind each figure.

Viewing results on GitHub

When you push this repository to GitHub and switch to the v2 branch, the plots under runs/ALL9/figures and runs/tables will be displayed automatically in the web UI. To ensure GitHub can find them, use relative links when referencing images in markdown or the report. For example, to embed the ALL9 CNN confusion matrix in your README or wiki page, use:

CNN confusion matrix

Similarly, the leaderboard CSV can be linked as:

Model comparison

GitHub will render these resources without any extra configuration. All figures used in the LaTeX report have been copied into the topâ€‘level figures/ and table/ folders to keep the manuscript selfâ€‘contained; the originals remain in runs/ for interactive browsing.

Training and evaluation

To reproduce the results:

Install dependencies â€“ create a Python environment and install packages listed in requirements.txt (PyTorch, torchvision, pandas, scikitâ€‘learn, etc.).

Run training using train_multilayer.py. For example, to train L1 ResNet50:

python train_multilayer.py --data_dir data_raw --phase L1 --model resnet50

Similarly, replace L1 with L2 or ALL9 and choose from resnet50, densenet121, mobilenetv3, efficientnet, cnn, vit_resnet, swin_densenet, vgg16 or vgg19.

Evaluate models with the scripts in eval_tools. The unified run_all_evals.py will run confusion matrices, ROC/PR, calibration, and write the results to runs/:

python -m eval_tools.run_all_evals --data_dir data_raw --phases ALL9 L1 L2 --models resnet50 densenet121 mobilenetv3 cnn vit_resnet swin_densenet efficientnet vgg16 vgg19

Generate the report â€“ compile main.tex with LaTeX (e.g. pdflatex or Overleaf). The report includes the figures from figures/ and tables from table/.

Launch the demo â€“ use streamlit run app.py to test the live prediction interface. The app loads your best models from runs/ and supports subclass prediction under Eczema, Fungal Infections and Pox.

Contributing

Fork this repository, create a branch (e.g. feature/new-model), commit your changes and submit a pull request. Please update the documentation and add new evaluation plots and tables under runs/ so that others can reproduce your results.

Acknowledgements

SkinBench v2 was developed at Daffodil International University as part of a bachelor thesis. We thank Bokkobidi Hospital for providing clinical guidance and the openâ€‘source community for the models used in this project.